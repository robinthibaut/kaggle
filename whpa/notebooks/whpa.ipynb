{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Wellhead Protection Area (WHPA) prediction #\n",
    "\n",
    "Author: Robin Thibaut, Ghent University.\n",
    "Date: 29/05/2022\n",
    "\n",
    "This is a multivariate regression problem.\n",
    "\n",
    "This dataset was created as part of the following study, which was published in the Journal of Hydrology: A new framework for experimental design using Bayesian Evidential Learning: the case of wellhead protection area [doi.org/10.1016/j.jhydrol.2021.126903](https://doi.org/10.1016/j.jhydrol.2021.126903).\n",
    "\n",
    "The pre-print is available on arXiv: [arxiv.org/pdf/2105.05539.pdf](https://arxiv.org/pdf/2105.05539.pdf)\n",
    "\n",
    "### Introduction ###\n",
    "The Wellhead Protection Area (WHPA) is a zone around a pumping well where human activities are limited in order to preserve water resources, usually based on how long dangerous chemicals in the area will take to reach the pumping well (according to local regulation). The flow velocity in the subsurface around the well determines it, and it can be computed numerically using particle tracking or transport simulation, or in practice using tracer testing. A groundwater model is typically calibrated against field data before being used to calculate the WHPA. In highly populated places where land occupation is a big concern, the introduction of such zones could have a large socioeconomic impact.\n",
    "\n",
    "### WHPA prediction ###\n",
    "Different tracers emerge from six data sources (injection wells) scattered across the pumping well. Their job is to inject individual tracers into the system in order to predict their transport and record their breakthrough curves (BCs) at the pumping well location.\n",
    "Numerous particles are artificially positioned around the pumping well, and their origins are traced backward in time to identify the associated WHPA.\n",
    "\n",
    "Our predictor and target were generated using the USGS' open-source finite-difference code Modflow. To get different sets of predictors and targets, we ran different hydrologic models with one variable parameter, namely hydraulic conductivity in metres per day. To obtain a satisfactory heterogeneity in the hydraulic conductivity fields, which control the shape and extent of our target, the PAs, we used sequential gaussian simulation based on arbitrarily defined variograms. The pumping well is located at the 1000, 500 metres mark and is surrounded by six injection wells.\n",
    "\n",
    "### Files description ###\n",
    "This dataset contains 4148 simulation results, i.e., 4148 pairs of predictor/target.\n",
    "`bkt.npy` contains the breakthrough curves from all 6 injection wells recorded at the pumping well.\n",
    "`pz.npy` contains the 2D coordinates of the backtracked particles' end points, used to delineate the WHPA."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Structure of the notebook ##\n",
    "1. Data loading\n",
    "2. Data preprocessing\n",
    "3. Model training\n",
    "4. Model prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's begin by importing the necessary libraries."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n",
    "# !pip install matplotlib>=3.3.4\n",
    "# !pip install scipy>=1.6.1\n",
    "# !pip install numpy>=1.20.1\n",
    "# !pip install scikit-fmm>=2021.2.2\n",
    "# !pip install skbel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the dataset ##\n",
    "Each instance is stored in a folder named with an `uuid`. The `predictor` (or `X`) is stored in a `bkt.npy` file. The `target` (or `y`) is stored in a `pz.npy` file.\n",
    "\n",
    "The aim of this notebook is to predict the `target` (`y`) from the `predictor` (`X`)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_path = '../datasets/forwards'  # Path to the data\n",
    "folders = os.listdir(data_path)  # list of folders\n",
    "bkt_files = []  # Breakthrough curves files\n",
    "sd_files = []  # Protection zone (WHPA) files\n",
    "for folder in folders:  # Loop over the folders\n",
    "    # check if it is a folder\n",
    "    if os.path.isdir(os.path.join(data_path, folder)):\n",
    "        bkt_files.append(os.path.join(data_path, folder, 'bkt.npy'))  # Add the bkt file\n",
    "        sd_files.append(os.path.join(data_path, folder, 'pz.npy'))  # Add the pz file\n",
    "\n",
    "# Now load all the data\n",
    "X = list(map(np.load, bkt_files))  # we use list to make it a list of numpy arrays\n",
    "y = np.array([np.load(f) for f in sd_files])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's have a look at the dataset  ###"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot the first instance of the predictor ####\n",
    "Here's an animation of how predictors are collected.\n",
    "\n",
    "Six tracers are injected by six wells.\n",
    "\n",
    "They will seep through the aquifer defined by a heterogeneous hydraulic conductivity field and their breakthrough curves at the pumping well are recorded over time.\n",
    "\n",
    "![Predictor generation](https://raw.githubusercontent.com/robinthibaut/kaggle/master/whpa/datasets/transport_animation_new.gif)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# let's have a look at the first instance\n",
    "# print(X[0].shape)\n",
    "# out: (6, 20933, 2)\n",
    "# 6 is the number of curves (one from each well)\n",
    "# 20933 is the number of time steps in each curve (varies from instance to instance)\n",
    "# 2 indicates the time and concentration\n",
    "for i in range(6):\n",
    "    plt.plot(X[0][i, :, 0], X[0][i, :, 1], label=f'curve {i+1}')\n",
    "plt.xlabel('Time (days)')\n",
    "plt.ylabel('Concentration (g/$m^3$)')\n",
    "plt.legend()\n",
    "plt.title('Breakthrough curves of the first instance')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot the first instance of the target ####"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(y[0].shape)\n",
    "# out: (145, 2)\n",
    "# 145 is the number of particles (vertexes)\n",
    "# 2 indicates X and Y coordinates\n",
    "# let's have a look at the first instance (scatter plot)\n",
    "plt.scatter(y[0][:, 0], y[0][:, 1], label='endpoints')\n",
    "# let's add the pumping well\n",
    "plt.scatter(1000, 500, label='pumping well')\n",
    "plt.xlabel('X (m)')\n",
    "plt.ylabel('Y (m)')\n",
    "plt.legend()\n",
    "plt.title('Protection zone of the first instance')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing ##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing the predictor ###"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "def curve_interpolation(tc0, n_time_steps: int = 200, t_max: float = 1.01080e02):\n",
    "    \"\"\"\n",
    "    Perform data transformations on the predictor.\n",
    "    The breakthrough curves do not share the same time steps.\n",
    "    We need to save the data array in a consistent shape, thus interpolates and subdivides each simulation curves into n time steps.\n",
    "    :param tc0: original data - breakthrough curves of shape (n_sim, n_wells, n_time_steps, 2)\n",
    "    :param n_time_steps: float: desired number of time step, will be the new dimension in shape[1].\n",
    "    :param t_max: float: Time corresponding to the end of the simulation (default unit is seconds).\n",
    "    :return: Observation data array with shape (n_sim, n_time_steps, n_wells)\n",
    "    \"\"\"\n",
    "    # Preprocess d\n",
    "    f1d = []  # List of interpolating functions for each curve\n",
    "    for t in tc0:\n",
    "        fs = [interp1d(c[:, 0], c[:, 1], fill_value=\"extrapolate\") for c in t]  # interpolate each curve\n",
    "        f1d.append(fs)\n",
    "    f1d = np.array(f1d)\n",
    "    # n_time_steps = 200  Arbitrary number of time steps to create the final transport array\n",
    "    ls = np.linspace(0, t_max, num=n_time_steps)\n",
    "    tc = []  # List of interpolating functions for each curve\n",
    "    for f in f1d:\n",
    "        ts = [fi(ls) for fi in f]\n",
    "        tc.append(ts)\n",
    "    tc = np.array(tc)  # Data array\n",
    "\n",
    "    return tc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# let's apply the function to the predictor set\n",
    "X_interp = curve_interpolation(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# let's have a look at the first instance of the new predictor\n",
    "print(X_interp[0].shape)\n",
    "# out: (6, 200)\n",
    "# 6 is the number of curves (one from each well)\n",
    "# 200 is the new number of time steps in each curve\n",
    "# not that we got rid of the time dimension. We are allowed to do so as the whole set is now interpolated over the same time steps.\n",
    "for i in range(6):\n",
    "    plt.plot(X_interp[0][i, :], label=f'curve {i+1}')\n",
    "plt.xlabel('Time (days)')\n",
    "plt.ylabel('Concentration (g/$m^3$)')\n",
    "plt.legend()\n",
    "plt.title('Breakthrough curves of the first instance after interpolation')\n",
    "# Observe that the curves are now interpolated over the same time steps and the time dimension is gone.\n",
    "# We also got rid of some noise in the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we concatenate the 6 curves into a single array of shape (n_sim, 6*200). Feel free to try with different combinations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# let's concatenate the 6 curves into a single array of shape (n_sim, 6*200)\n",
    "X_interp = np.concatenate(X_interp.reshape(6, -1, 200), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# let's have a look at the first instance of the new predictor\n",
    "print(X_interp.shape)\n",
    "# out: (6*200,)\n",
    "# 6*200 is the new number of time steps in each curve\n",
    "plt.plot(X_interp[0], label='curves 1 to 6')\n",
    "plt.ylabel('Concentration (g/$m^3$)')\n",
    "plt.legend()\n",
    "plt.title('Breakthrough curves of the first instance after concatenation')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing the target ###\n",
    "Preprocessing the target is a bit more complicated as we have to deal with the fact that the target is a set of endpoints.\n",
    "\n",
    "It does not really make sense to try to predict the 2D position of the endpoints, as the 2D position is not a continuous function.\n",
    "\n",
    "Instead, we need to find a way to represent the endpoints as a continuous function.\n",
    "\n",
    "The endpoints have already been linked together using the Traveling Salesman Problem to form the delineation of the protection zone. We used Google's OR-Tools library. We can use those paths to create a continuous function with the Signed Distance Function.\n",
    "\n",
    "Let's first define our functions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import skfmm\n",
    "\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "from skbel.spatial import block_shaped\n",
    "\n",
    "def binary_polygon(\n",
    "    xys: np.array,\n",
    "    nrow: int,\n",
    "    ncol: int,\n",
    "    pzs: np.array,\n",
    "    outside: float = -1,\n",
    "    inside: float = 1,\n",
    ") -> np.array:\n",
    "    \"\"\"\n",
    "    Given a polygon whose vertices are given by the array pzs, and a matrix of\n",
    "    centroids coordinates of the surface discretization, assigns to the matrix a certain value\n",
    "    whether the cell is inside or outside said polygon.\n",
    "    To compute the signed distance function, we need a negative/positive value.\n",
    "    :param xys: Centroids of a grid' cells\n",
    "    :param nrow: Number of rows\n",
    "    :param ncol: Number of columns\n",
    "    :param pzs: Array of ordered vertices coordinates of a polygon.\n",
    "    :param pzs: Polygon vertices (v, 2)\n",
    "    :param outside: Value to assign to the matrix outside the polygon\n",
    "    :param inside: Value to assign to the matrix inside the polygon\n",
    "    :return: phi = the binary matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # Creates a Polygon abject out of the polygon vertices in pzs\n",
    "    poly = Polygon(pzs, True)\n",
    "    # Checks which points are enclosed by polygon.\n",
    "    ind = np.nonzero(poly.contains_points(xys))[0]\n",
    "    phi = np.ones((nrow, ncol)) * outside  # SD - create matrix of 'outside'\n",
    "    phi = phi.reshape((nrow * ncol))  # Flatten to have same dimension as 'ind'\n",
    "    phi[ind] = inside  # Points inside the WHPA are assigned a value of 'inside'\n",
    "    phi = phi.reshape((nrow, ncol))  # Reshape\n",
    "\n",
    "    return phi\n",
    "\n",
    "def h_sub(h: np.array, un: int, uc: int, sc: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Process signed distance array.\n",
    "    :param h: Signed distance array\n",
    "    :param un: number of rows\n",
    "    :param uc: number of columns\n",
    "    :param sc: New cell dimension in x and y direction (original is 1)\n",
    "    :return: New signed distance array\n",
    "    \"\"\"\n",
    "    h_u = np.zeros((h.shape[0], un, uc))\n",
    "    for p in range(h.shape[0]):\n",
    "        sim = h[p]\n",
    "        sub = block_shaped(arr=sim, nrows=sc, ncols=sc)\n",
    "        h_u[p] = np.array([s.mean() for s in sub]).reshape(un, uc)\n",
    "    return h_u\n",
    "\n",
    "\n",
    "def signed_distance(xys: np.array, nrow: int, ncol: int, grf: float, pzs: np.array):\n",
    "    \"\"\"\n",
    "    Given an array of coordinates of polygon vertices, computes its signed distance field.\n",
    "    :param xys: Centroids of a grid' cells\n",
    "    :param nrow: Number of rows\n",
    "    :param ncol: Number of columns\n",
    "    :param grf: Grid dimension (uniform grid)\n",
    "    :param pzs: Array of ordered vertices coordinates of a polygon.\n",
    "    :return: Signed distance matrix\n",
    "    \"\"\"\n",
    "    phi = binary_polygon(xys, nrow, ncol, pzs) # phi is a binary matrix\n",
    "    sd = skfmm.distance(phi, dx=grf)  # Signed distance computation\n",
    "    return sd\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's a lot of processing. Let's now apply it to our data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from skbel.spatial import grid_parameters\n",
    "\n",
    "# First, we need to define the grid parameters\n",
    "# Geometry of the focused area on the main grid, enclosing all wells, as to reduce computation time\n",
    "x_range = [800, 1150]  # x-range (m)\n",
    "y_range = [300, 700]  # y-range (m)\n",
    "# Defines cell dimensions for the signed distance computation.\n",
    "cell_dim = 4  # Cell dimension (m)\n",
    "\n",
    "xys, nrow, ncol = grid_parameters(\n",
    "    x_lim=x_range, y_lim=y_range, grf=cell_dim\n",
    ")\n",
    "# xys: Centroids of a grid' cells\n",
    "# nrow: Number of rows\n",
    "# ncol: Number of columns\n",
    "\n",
    "# Compute signed distance on pzs.\n",
    "# h is the matrix of target feature on which PCA will be performed.\n",
    "y_sd = np.array(\n",
    "    [signed_distance(xys, nrow, ncol, cell_dim, pp) for pp in y]\n",
    ")\n",
    "\n",
    "# It is important to note that the shape of the matrix is (ncol, nrow) = (87, 100)\n",
    "# For training purposes, we will flatten the matrix to have a vector of shape (n_sim, 87*100)\n",
    "y_sd_flat = y_sd.reshape((y_sd.shape[0], nrow * ncol))\n",
    "print(y_sd_flat.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's examine the transformed target `y_sd`.\n",
    "\n",
    "Each instance is now a matrix of signed distance values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(y_sd[0].shape)\n",
    "# out: (100, 87) = (nrow, ncol)\n",
    "# Let's plot the first instance\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(y_sd[0], cmap=\"coolwarm\", origin=\"lower\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Signed distance field of the first instance\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Can you recognize the shape of the target?\n",
    "\n",
    "Let's plot the first instance in 2D on top of the grid.\n",
    "\n",
    "The target is now represented as the 0 contour of the signed distance field."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(y_sd[0], cmap=\"coolwarm\", origin=\"lower\", extent=[x_range[0], x_range[1], y_range[0], y_range[1]])  # Plot the signed distance field over the grid\n",
    "plt.colorbar()\n",
    "plt.plot(y[0][:, 0], y[0][:, 1], \"ko\", markersize=2)  # plot the original endpoints\n",
    "plt.contour(y_sd[0], levels=[0], colors=\"black\", extent=[x_range[0], x_range[1], y_range[0], y_range[1]])  # plot the 0 contour of the signed distance field\n",
    "plt.title(\"Signed distance field of the first instance\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the model ##\n",
    "This is where you should start being creative. You can of course go back to an earlier stage and choose a different preprocessing method.\n",
    "\n",
    "You can try different models, different hyperparameters, different preprocessing, etc.\n",
    "\n",
    "The following will be the application of our Bayesian Evidential Learning framework as described in the paper. The framework is described in more detail in the paper.\n",
    "\n",
    "Here's a quick overview of the framework.\n",
    "1. Preprocess the data\n",
    "2. Apply dimensionality reduction (PCA)\n",
    "3. Train the model with Canonical Correlation Analysis (CCA)\n",
    "4. Apply the model to the test data\n",
    "5. Evaluate the performance of the model\n",
    "6. Repeat the process for different models and hyperparameters\n",
    "\n",
    "It is important to note that as a bayesian method, we do not predict one single value, but rather the posterior distribution of the target given the observed data. We can then sample as many times as we want from the posterior distribution to obtain our predictions.\n",
    "\n",
    "The steps are illustrated in the following figure.\n",
    "\n",
    "![BEL](https://raw.githubusercontent.com/robinthibaut/kaggle/master/whpa/datasets/belf.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from skbel import BEL\n",
    "\n",
    "# Pipeline before CCA\n",
    "X_pre_processing = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"pca\", PCA(n_components=50)),\n",
    "    ]\n",
    ")\n",
    "Y_pre_processing = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"pca\", PCA(n_components=30)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Canonical Correlation Analysis\n",
    "cca = CCA(n_components=30)\n",
    "\n",
    "\n",
    "# Initiate BEL object\n",
    "model = BEL(\n",
    "    X_pre_processing=X_pre_processing,\n",
    "    Y_pre_processing=Y_pre_processing,\n",
    "    cca=cca,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's first split the data into training and test sets.\n",
    "\n",
    "We will use the first 80% of the data for training and the remaining 20% for testing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_interp, y_sd_flat, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#  Set model parameters\n",
    "model.mode = \"tm\"  # How to compute the posterior conditional distribution (tm: transport map)\n",
    "model.random_state = 42  # Random state\n",
    "# Save original dimensions of both predictor and target\n",
    "model.Y_shape = (100, 87)  # 100 rows and 87 columns\n",
    "# Number of samples to be extracted from the posterior distribution\n",
    "model.n_posts = 500"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fit BEL model\n",
    "model.fit(X=X_train, Y=y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The posterior distribution is computed within the method below.\n",
    "# Let's only predict the first instance (index 0)\n",
    "n_obs = 0\n",
    "y_predicted = model.predict(X_test[n_obs], inverse_transform=True)\n",
    "print(y_predicted.shape)\n",
    "# out: (1, n_posts, 8700)\n",
    "# For one example, we sampled 400 posterior samples from the model and backtransformed them to the original space."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's visualize the results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the predicted WHPAs (O contour of the signed distance fields)\n",
    "plt.figure(figsize=(20, 20))\n",
    "# plot the training set in the background\n",
    "for pct in y_train:\n",
    "    plt.contour(pct.reshape(model.Y_shape), levels=[0], alpha=.2, colors=\"blue\", linewidths=1, extent=[x_range[0], x_range[1], y_range[0], y_range[1]])\n",
    "# plot the predicted WHPAs\n",
    "for pc in y_predicted[0]:\n",
    "    pcr = np.reshape(pc, model.Y_shape)\n",
    "    plt.contour(pcr, levels=[0], colors=\"green\", alpha=.2, extent=[x_range[0], x_range[1], y_range[0], y_range[1]])  # plot the 0 contour of the signed distance field\n",
    "# Plot the true WHPA\n",
    "plt.contour(y_test[n_obs].reshape(model.Y_shape), levels=[0], colors=\"red\", alpha=1, linewidths=3,extent=[x_range[0], x_range[1], y_range[0], y_range[1]])  # plot the 0 contour of the true signed distance fied"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}